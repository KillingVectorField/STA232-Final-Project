---
title: "gogogo"
author: "Xiancheng Lin"
date: "12/1/2019"
output: pdf_document
---

```{r}
library(car)
library(stringr)
library(leaps)
library(MASS)
library(corrplot)
data<-read.table("auto-mpg.txt",col.names = c("mpg","cylinders","displacement",
                                              "horsepower","weight","acceleration",
                                              "year","origin","name"),
                 colClasses = c("numeric","integer","numeric",
                                "numeric","numeric","numeric","integer","factor","character"),na.strings = "?")

data[,9]=word(data[,9],1)
```
First, we can get a brief description of our data
```{r}
head(data)
dim(data)
summary(data)
```
First,we need to decide whether take name into consideration.So, we divide the data into different groups by name, and count the number of data in each group.
```{r}
aggregate(mpg~name,data=data,length)  
```
According to the result,we do not have enough dataset for each group, so we delete this predictor "name".
```{r}
data=data[,-9]
```
Second, we consider the predictors "year" and "origin"
```{r}
aggregate(mpg~year+origin,data=data,length)  ###not enough dataset for each group
```
We don't have enough data for each group,so we just pick origin as a factor into consideration.

Next, we can draw a picture of the predictors and explore their relationship.
```{r}
scatterplotMatrix(data,cex=0.6,col=1)
cor(na.omit(data[,1:7]))
corrplot(cor(na.omit(data[,-8])))
```
An advantage of scatterplotMatrix comparing  to pair is that we can see the density of predictors.

we can see that the predictors and response are mostly right skewed, so we can consider variables transformation later on.

Also, from the picture we can see that displacement and horsepower and weight seem to have strong multilinearity and they all have a inverse relationship with mpg.

```{r}
####model selection
data=data[,-c(7,8)]
regsubsets<-regsubsets(mpg~.,data=data)
sumreg<-summary(regsubsets)
sumreg
data.frame(adjr2=sumreg$adjr2,cp=sumreg$cp,bic=sumreg$bic)

###Consider two variables to do regression
lm.fit1<-lm(mpg~horsepower+weight,data=na.omit(data))
summary(lm.fit1)

###residual analysis
#step1  normality
e1=residuals(lm.fit1)
yhat1=fitted(lm.fit1)
plot(yhat1,e1)
resid.lowess=lowess(yhat1,e1,f=0.8)
lines(resid.lowess,col=2)
###from the plot we can see that the residuals are not linear, and has a tendency of non-linear
qqnorm(e1)
qqline(e1)


boxcox(mpg ~ horsepower+weight,data=data)
boxcox(horsepower~mpg+weight,data=data)
boxcox(weight~mpg+horsepower,data=data)

lm.fit2=lm(mpg^(-1/2)~log(horsepower)+log(weight),data=na.omit(data))
summary(lm.fit2)

e2=residuals(lm.fit2)
yhat2=fitted(lm.fit2)
plot(yhat2,e2)
resid.lowess=lowess(yhat2,e2,f=0.8)
lines(resid.lowess,col=2)
###from the plot we can see that the residuals are not linear, and has a tendency of non-linear
qqnorm(e2)
qqline(e2)

###influence measure
par(mfrow=c(3,2))
plot(lm.fit2,which=1:6)
mean(abs(lm.fit2$residuals))
#consider the possible high leverage and outlier points 29,155,156

data=data[-c(29,155,156),]
lm.fit3=lm(mpg^(-1/2)~log(horsepower)+log(weight),data=na.omit(data))
summary(lm.fit3)
mean(abs(lm.fit3$residuals))

##we can see that after deleting the high influence points, we enhance the adjusted R2
## and reduce the average absolute residuals.
###multilinear 
vif(lm.fit3)

#they are both lower than 10, so we believe there is no multilinearlity.



###observe that horsepower and weight have the same transformation and their coefficients
###are almost the same, so consider a new predictor horsepower*weight,named new
data[,7]=data[,4]*data[,5]
colnames(data)[7]="new"
boxcox(mpg~new,data=data) ###inverse transformation
boxcox(new~mpg,data=data) ###log transformation
lm.fit4=lm(1/mpg~log(new),data=na.omit(data))
summary(lm.fit4)
mean(abs(lm.fit4$residuals))
e4=residuals(lm.fit4)
yhat4=fitted(lm.fit4)
plot(yhat4,e4)
resid.lowess=lowess(yhat4,e4,f=0.8)
lines(resid.lowess,col=2)
qqnorm(e4)
qqline(e4)

```
Compare lm.fit4 to lm.fit3, lm.fit4 reduce the absolute mean of residuals significantly without a much reduce on R2, so maybe lm.f4 is now the best model we get.